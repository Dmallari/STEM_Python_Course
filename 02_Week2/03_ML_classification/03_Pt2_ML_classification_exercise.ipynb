{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Pt2_ML_classification_exercise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZnjFtKPqhcz",
        "colab_type": "text"
      },
      "source": [
        "# Python for STEM - Week 2 (Advanced)  \n",
        "\n",
        "## Day 3 - Part 2: Supervised learning - classification\n",
        "\n",
        "In this notebook, we will focus on examples of supervised machine learning. More specifically, we will be doing classification using Scikit-learn, one of the machine learning packages in Python. Before we start, here we first import all the packages that we need for this notebook. \n",
        "\n",
        "All the machine learning functions we will use in Day 3 and Day 4 all comes from [scikit-learn](https://scikit-learn.org/stable/index.html). You can find very detailed descriptions on many machine learning models included in the package user guide and various examples. This would be a good place to start when you want to adopt machine learning for your own research/work.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkWCpoG4qasM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## In this cell, we import all the packages needed for this notebook\n",
        "import numpy as np                  ## packages for data handling\n",
        "import pandas as pd                 \n",
        "from scipy.spatial.distance import cdist \n",
        "import matplotlib.pyplot as plt     ## packages for visualization \n",
        "import seaborn as sbn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J9Lh3hnqdHQ",
        "colab_type": "text"
      },
      "source": [
        "## Data ingest  \n",
        "In this notebook, we will use a dataset from geography/remote sensing. The data includes 1875 data points (locations) in the western North Carolina (Asheville region). Each row of the data contains the information of a point with its latitude/longitude, land cover type (forest, crop, urban, or water), and the surface reflectance data of six channels from the OLI sensor onboard USGS/NASA Land resource satellite Landsat-8. The reflectance data provide unique feature of the land surface as seen by the satellite sensor, which allows geographer understand how the surface is changing through time. To find more information about the data and satellite, you can visit USGS website about [Landsat-8 OLI data.](https://www.usgs.gov/land-resources/nli/landsat/landsat-8?qt-science_support_page_related_con=0#qt-science_support_page_related_con)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c948Q9kIsH_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## First we use pandas to read in the comma separated values (CSV) file\n",
        "datafile = 'https://raw.githubusercontent.com/uofscphysics/STEM_Python_Course/Summer2020/02_Week2/Data/03_land_use_land_cover_asheville.csv'\n",
        "AVLData = pd.read_csv(datafile, index_col=None)\n",
        "\n",
        "## We will check the first five rows of the data to have initial understadning of our data\n",
        "print( AVLData.head() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twKa9vtGvC_H",
        "colab_type": "text"
      },
      "source": [
        "In this dataset, **Class** refers to the land cover type, **B1** ~ **B6** are the surface reflectances of the sixe OLI channels. The table below explains what the class code represents.\n",
        "\n",
        "| Class No. | Land Cover Type |\n",
        "|-:|-:|\n",
        "|0|Forest| \n",
        "|1|Crop|\n",
        "|2|Development/Urban|\n",
        "|3|Water|\n",
        "\n",
        "Additionally, the following table gives us a quick explaination of what are the six OLI channels.  \n",
        "\n",
        "| Channel No. | Channel Name | Wavelength |\n",
        "|-:|-:|:-:|\n",
        "|B1|Coastal/Areasol|0.433 – 0.453 μm| \n",
        "|B2|Blue|0.450 – 0.515 μm|\n",
        "|B3|Green|0.525 – 0.600 μm|\n",
        "|B4|Red|0.630 – 0.680 μm|\n",
        "|B5|Near Infrared|0.845 – 0.885 μm|\n",
        "|B6|Short Wavelength Infrared|1.560 – 1.660 μm|  \n",
        "\n",
        "Typically, refletance value is between 0 and 1, describing the percentage of light reflected by the surface. The reflectance value in our data is the scaled value between 0 and 10000. You can simply convert it back to regular reflectance by multiplying 0.0001.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wimfgdw5ySx1",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we are doing clustering, so the **Class** information is not relevant because we are trying to guess how many clusters that we have based on this dataset. So let's assume that we do not have the land cover information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QusheEMHuqPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Now we have a simple matrix with six columns (attributes)\n",
        "## X is the new pandas data.frame with only the six channel reflectance\n",
        "X = AVLData.iloc[:,3:]\n",
        "print( X.head() )\n",
        "## We are now looking at the pairwise scatter plots between these six channels\n",
        "## using seaborn.pairplot function to look at them in one bix plot.\n",
        "## We are using \"alpha\" key word to change the transparency for the dots since\n",
        "## there are many overlapping amongst the data.\n",
        "sbn.pairplot(X, diag_kind = 'kde',\n",
        "             plot_kws = {'alpha': 0.5, 's': 30, 'edgecolor': 'k'},\n",
        "             height = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY55QlVSVJ-0",
        "colab_type": "text"
      },
      "source": [
        "## Feature transformation/extraction\n",
        "\n",
        "As we can see from the pairwise scatter plots, the current six channels share some strong correlation amongst them. Can we find more useful features based on these six original channels? \n",
        "\n",
        "Feature transformation/extraction is the process to reduce the dimensionality of the data to explain the most of the variances in the data. Principle Component Analysis (PCA) is one of these techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJsDigGOypOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Using PCA function from scikit-learn to perform feature transformation\n",
        "from sklearn.decomposition import PCA\n",
        "pca_AVL = PCA(n_components=6)\n",
        "principalComponents_X = pca_AVL.fit_transform(X)\n",
        "print( principalComponents_X )\n",
        "## The outcome of PCA is a ndarry here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5KKAs1CX6rF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## we convert the ndarry to pandas data.frame with specified column names\n",
        "## for PCA\n",
        "PCA_df_X = pd.DataFrame(principalComponents_X, \n",
        "                        columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6'])\n",
        "print( PCA_df_X )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82G6JqPO3PaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We now want to know how much variance of the data is explained by each\n",
        "## of the principle components.\n",
        "with np.printoptions(precision=4, suppress=True):\n",
        "    print('Explained variation per principal component: {}'.\\\n",
        "          format(pca_AVL.explained_variance_ratio_) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8As6B9BCXm1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We can now visualize the first three components via the pairwise scatter \n",
        "## plot that we have done earlier.\n",
        "sbn.pairplot(PCA_df_X.iloc[:,:3], diag_kind = 'kde',\n",
        "             plot_kws = {'alpha': 0.5, 's': 80, 'edgecolor': 'k', 'color': 'darkgreen'},\n",
        "             height = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnQT1kl3dKPw",
        "colab_type": "text"
      },
      "source": [
        "## Data spliting\n",
        "\n",
        "For supervised learning, data spliting is the first step in order to develop and evaluation your model. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3COORCzddaZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Import function for data spliting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## We are spliting our data set into training and testing sets based on a 80:20 ratio\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGId8GFaek_A",
        "colab_type": "text"
      },
      "source": [
        "When we are spliting the data, we want to make sure the training and testing dataset can resemble the distribution of these four classes in order to make sure our data can represent the reality.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD4iMwXNpmoT",
        "colab_type": "text"
      },
      "source": [
        "## k-Nearest neighbors (kNN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf2z0Je74SNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Import packages for k-nearest neighbor classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Initialize a KNN classifier object. Later, we'll train it and then have it predict the\n",
        "# classes of withheld testing samples.\n",
        "\n",
        "\n",
        "# let's train the model with k=5\n",
        "\n",
        "\n",
        "\n",
        "# Have the newly trained classifier predict the classes of the withheld testing data.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwfV4cecgIDN",
        "colab_type": "text"
      },
      "source": [
        "Now we have a kNN classifier generated. We need to evaluate how the model performs based on our data. We can tabulate the predicted classes and the reference classes into a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0xOL2Oapt42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "## Calculate the confusion matrix and normalized confusion matrix\n",
        "\n",
        "\n",
        "# Initialize figure, axes for the two confusion matrices.\n",
        "\n",
        "\n",
        "# Plot the raw counts' confusion matrix.\n",
        "\n",
        "\n",
        "# Add labels to the x-axis and the y-axis.\n",
        "\n",
        "\n",
        "# Plot the percentages' confusion matrix.\n",
        "\n",
        "\n",
        "# Add a label to the x-axis.\n",
        "\n",
        "# Add a title to the figure.\n",
        "\n",
        "# Display the figure.\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LWBThgFhzF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Print the accuracy matrix for the classifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_upVzF9uhJLw",
        "colab_type": "text"
      },
      "source": [
        "## Cross validation\n",
        "\n",
        "If you have noticed, we fix the k-value for this k-Nearest neighbor classifier (k=9). \n",
        "\n",
        "1. Can we improve our overall accuracy by changing our k value in the model trianing? \n",
        "\n",
        "2. How can we decide which is the best k value for the model?  \n",
        "\n",
        "To answer these questions, we need to use cross validation, a process used for finding the best model hyperparameters for a machine learning model (e.g., k for k-nearest neighbors).\n",
        "\n",
        "To explore the optimum combination of the model hyperparameters, we typically uses the cross validation (CV) strategy. There are different type of cross validation strategy, such as, k-fold cross validation, leave-one-out cross validation, repeated cross validation. More information can be found in this [well written article about the importance of CV for machine learning](https://www.digitalvidya.com/blog/cross-validation-in-machine-learning/).  \n",
        "\n",
        "The most commonly used CV strategy is usually [k-fold CV](https://machinelearningmastery.com/k-fold-cross-validation/). But it still depends on the data that have for your problem. If you are using time series data, your cross validation strategy will be different since you want to account for the tempooral autocorrelation within your data. Same idea applies when your data have strong spatial auto correlation. But today, let's assume that k-fold CV can solve our problem. \n",
        "\n",
        "The most commonly used CV strategy is usually k-fold CV. But it still depends on the data that have for your problem. If you are using time series data, your cross validation strategy will be different since you want to account for the tempooral autocorrelation within your data. Same idea applies when your data have strong spatial auto correlation. But today, let's assume that k-fold CV can solve our problem.\n",
        "\n",
        "We can use a stratified 10-fold CV to find the optimum hyperparameters for our two classifiers (i.e., k-NN and random forest) to see if the model performance could be further improved. Let's start with k-NN to find out the best number of neighbors (k) for the model.\n",
        "\n",
        "The idea is to repeat the model traininig 10 times using a subset (9/10) of the training data set with different value of k. The average value of the model performance metrices from these 10 iteration of model training is used to choose which hyperparameter (i.e., k) performs best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x2-ZZPKzfgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Set up the parameter grid. In this case, it's just a list of all the\n",
        "# candidate number of neighbors we'll consider.\n",
        "\n",
        "# Initialize a KNN classifier object. \n",
        "\n",
        "\n",
        "# Initialize a stratified 10-fold generator for the cross-validation.\n",
        "\n",
        "# Create the grid search object. This object will take the kNN classifier\n",
        "# and run stratified 10-fold cross-validation for each of the potential\n",
        "# candidates for k. It will record the averaged accuracy for each k so\n",
        "# that afterwards we can view how the classifier's accuracy improves or\n",
        "# worsens with respect to k.\n",
        "\n",
        "# Run the grid search for the optimal number of neighbors, k.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOPm2v0qji-P",
        "colab_type": "text"
      },
      "source": [
        "We now have a set of models and its performance based on this stratified 10-fold cross validation. Now, we can plot the model accuracy as a function of *k*. This will assist us selecting the optimum solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edERAaT-jluG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Retrieving the mean and standard deviation of the accuracy for the model test scores\n",
        "\n",
        "## Plotting out the accuracy agiand k\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5-DpbAnj7aN",
        "colab_type": "text"
      },
      "source": [
        "Now we know that the kNN model performs best when we use k value of 9 based on our 10-fold cross validation strategy. Now we can safely train out final kNN model with k of 9 and apply it to our independent testing data that we set aside at the begining of this process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08SG-FRnkPKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a KNN classifier object. Later, we'll train it and then have it predict the\n",
        "# classes of withheld testing samples.\n",
        "\n",
        "# Have the final classifier predict the classes of the withheld testing data.\n",
        "\n",
        "# Let's print out the accuracy indicators\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbJrI8YCk8_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Now, we can also visualize the testing confusion matrix\n",
        "## Calculate the confusion matrix and normalized confusion matrix\n",
        "\n",
        "# Initialize figure, axes for the two confusion matrices.\n",
        "\n",
        "# Plot the raw counts' confusion matrix.\n",
        "\n",
        "# Add labels to the x-axis and the y-axis.\n",
        "\n",
        "# Plot the percentages' confusion matrix.\n",
        "\n",
        "# Add a label to the x-axis.\n",
        "\n",
        "\n",
        "# Add a title to the figure.\n",
        "\n",
        "\n",
        "# Display the figure.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_S-UOoQlloN",
        "colab_type": "text"
      },
      "source": [
        "## Random forest  \n",
        "\n",
        "### 2.3 - Random forest  \n",
        "\n",
        "There are more sophiscated models that we can use to address more sophisicated classification problems. Here, we choose to introduce [random forest](https://en.wikipedia.org/wiki/Random_forest) model because of its robust performance in many applications. The random forest is an ensemble learning model by creating a suite of decision trees at training time and outputting the class that is the majority of the classes from each individual tree for classification problem. The fundamental idea of random forest  model is that the **collective power of multiple \"weak\" models can outperform any individual \"strong\" model**. It can address overfitting issue comparing to the regular decision tree model.  \n",
        "\n",
        "Similar with k-NN, there are different flavors of random forest model. We are using the classic random forest model with *scikit-learn* package in Python (i.e., [_RandomForestClassifier_](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees). As the model gets more complicated, we have more to consider about the model structure, such as, how many variables we want to have as input for each tree (*max_features*), number of trees (*n_estimator*), the depth of a decision tree (*max_depth*, *min_sample_split*, *min_sample_leaf*). These could all have potential impact on our final model outcomes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H7Hrwxylwvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the random forest classifier from sklearn.ensemble\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Set up the parameter grid. In this case, it's just a list of all the\n",
        "# candidate number of neighbors we'll consider.\n",
        "\n",
        "\n",
        "# Initialize a ra classifier object. \n",
        "\n",
        "\n",
        "# Create the grid search object. This object will take the kNN classifier\n",
        "# and run stratified 10-fold cross-validation for each of the potential\n",
        "# candidates for k. It will record the averaged accuracy for each k so\n",
        "# that afterwards we can view how the classifier's accuracy improves or\n",
        "# worsens with respect to k.\n",
        "\n",
        "\n",
        "# Run the grid search for the optimal number of neighbors, k.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptD1txhymaP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Print best parameter combination\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH2LNi3emf6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Calculating the mean and standard deviation of the accuracy for the model test scores\n",
        "\n",
        "\n",
        "## rearrange the accuracy data to a 2d array so we can have a heatmap\n",
        "\n",
        "## 5 rows for max_features & 7 columns for min_samples_leaf\n",
        "\n",
        "## Creating the heatmap for both mean accuracy and standard deviation\n",
        "# Initialize figure, axes for the accuracy heatmap.\n",
        "\n",
        "\n",
        "# Plot the raw counts' confusion matrix.\n",
        "\n",
        "\n",
        "\n",
        "# Add labels to the x-axis and the y-axis.\n",
        "\n",
        "\n",
        "# Plot the percentages' confusion matrix.\n",
        "\n",
        "\n",
        "\n",
        "# Add labels to the x-axis and the y-axis.\n",
        "\n",
        "\n",
        "# Add a title to the figure.\n",
        "\n",
        "\n",
        "# Display the figure.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QisdYFcnKya",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAE9xoWXnKEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train our final model based on our 10-fold cross validation results with the \n",
        "## best model hyperparameters\n",
        "\n",
        "\n",
        "# Using the training set, we can fit our model now\n",
        "\n",
        "\n",
        "# Have the newly trained classifier predict the classes of the withheld testing data.\n",
        "\n",
        "\n",
        "## reporting the indicators for the random forest classifier\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_37UfVNn2u0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Calculate the confusion matrix and normalized confusion matrix\n",
        "\n",
        "\n",
        "# Initialize figure, axes for the two confusion matrices.\n",
        "\n",
        "\n",
        "# Plot the raw counts' confusion matrix.\n",
        "\n",
        "\n",
        "\n",
        "# Add labels to the x-axis and the y-axis.\n",
        "\n",
        "\n",
        "# Plot the percentages' confusion matrix.\n",
        "\n",
        "\n",
        "\n",
        "# Add a label to the x-axis.\n",
        "\n",
        "\n",
        "# Add a title to the figure.\n",
        "\n",
        "\n",
        "# Display the figure.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G1m3znFoGST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}