{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Workshop_machine_learning_exercise",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyKkS9_rKXiQ",
        "colab_type": "text"
      },
      "source": [
        "## Introduction  \n",
        "\n",
        "In this mini workshop, we are going to learn some advanced features in [scikit-learn](https://scikit-learn.org/stable/user_guide.html) that can improve your undersanding and efficiency of machine learning.  \n",
        "\n",
        "This time, we will use a built-in dataset from scikit-learn as an example for our exercise. The dataset we are using is the [Boston house price](https://scikit-learn.org/stable/datasets/index.html#toy-datasets) dataset for regression modeling. \n",
        "\n",
        "Let's start the notebook with importing the package and the data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciogko39LjXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sbn\n",
        "from sklearn.datasets import load_boston"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAGuiY1sKPYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = load_boston(return_X_y=True)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNKe4xEoM53q",
        "colab_type": "text"
      },
      "source": [
        "It looks like we have 13 features that we  can use to create a predictive model for the target/output which is the house price in Boston."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEzEgwz9NYx0",
        "colab_type": "text"
      },
      "source": [
        "## Pipeline  \n",
        "\n",
        "In the past two days, we are doing the model development step-by-step with the typical process of:\n",
        "\n",
        "1. Feature selection/transformation;\n",
        "2. Defining the model;\n",
        "3. Tunning model hyperparameters;\n",
        "\n",
        "This is a clear way to start our learning of each individual step. However, the code will look lengthy. In scikit-learn, you can create a model [pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline-chaining-estimators) to nest all model steps into a sequence and specifying the key words for each step. You can also implement grid search CV directly to your pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74HpXTr2OgMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "## First, let's review a traditional modeling process\n",
        "\n",
        "## Step 1. Data transformation - e.g., PCA\n",
        "\n",
        "## Step 2. Defining the model\n",
        "\n",
        "## Step 3. Gridsearch CV\n",
        "\n",
        "\n",
        "## Step 4. fit the model/grid search\n",
        "\n",
        "## Step 5. find the best combo & final model\n",
        "\n",
        "## ...\n",
        "\n",
        "## Things can be simplified with pipeline by nesting these together\n",
        "\n",
        "\n",
        "## making the pipeline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBbvlkeKNXF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Now, le's say we want to change how many different PCA components we want to include in our final model \n",
        "## also, we want to change the number of estimators (trees) in our random forest\n",
        "\n",
        "\n",
        "## now we can define the gridsearch object that we want (say with 10-fold cross validation)\n",
        "\n",
        "## Seperating the data for training and testing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2BoaIOTWetS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Now we can fit the whole pipeline instead of just doing it step by step\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghzS3vOHX8KA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Let's looks at what is the suggestion of the model structure\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEzMaVcTfvjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Now we can directly apply the model pipeline from the grid search \n",
        "## object to our testing data which will give us the best estimator\n",
        "## based on our grid search CV results.\n",
        "\n",
        "\n",
        "## We can also add the scatter plot between predicted and test data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDcmPp82jalS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Now it's your turn to try build your own pipeline for building a model \n",
        "### for this Boston Housing Price data using neural network.\n",
        "### For the hyperparameters, you can change both the number of hidden layers\n",
        "### for 1-layer and 2-layer models, as well as the activation function\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52a74KpVjzfe",
        "colab_type": "text"
      },
      "source": [
        "## Random Search for Hyperparameters\n",
        "\n",
        "In the previous exercise, we always use the grid search method to find the best model hyperparameters. This is a good method when you have a limited number of hyperparameters and small range of the parameters to tune. However, when we have a large parameter space for searching, the grid search can be really time consuming for large data sets. Sometimes, random search can help you reduce the computational need for that. We are going to use random forest model as our base model again here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elGukx_ckc8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## define our base model pipeline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPBDMzSSk_JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from time import time\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
        "                  .format(results['mean_test_score'][candidate],\n",
        "                          results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "## In this situation, we can define a grid for our search\n",
        "\n",
        "\n",
        "## Now we can perform our grid search with 5-fold cross validation\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO2B-HU2nr8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Now let's see how the random search will take for us\n",
        "import scipy.stats as stats\n",
        "## here we define the range for search first\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvBdl6rjpr8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## run randomized search\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeKqF2zQpvAk",
        "colab_type": "text"
      },
      "source": [
        "## Assessing feature importance  \n",
        "\n",
        "Assuming that we finally find our best random forest model. We want to know which features have higer importance than others. What we can do is to use the [permutation feature importance](https://scikit-learn.org/stable/modules/permutation_importance.html#) functionality in scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1-t7VxZqWTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Now, let's use the model hyperparameter from our random search as the final model\n",
        "\n",
        "\n",
        "## Let's calculate feature importance here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHO7LMsTrQi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We want to visualize the ranking of individual features\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}