{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison / model selection\n",
    "## using the Akaike Information Criterion and Bayesian Information Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages we will need in this notebook\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in some data we created for this example (.dat is a generic filename, it's just a text file)\n",
    "data_filename='https://raw.githubusercontent.com/uofscphysics/STEM_Python_Course/Summer2020/02_Week2/Data/1D_intro_examples.dat'\n",
    "example_data_1D = pandas.read_csv(data_filename,sep=',',header=0)#this file is separated by spaces and its first line contains the names of the columns (header) \n",
    "print(example_data_1D.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the data, with error bars, that we read from file (See Day 2)\n",
    "plt.errorbar(example_data_1D['x'], #x,y,and error are the column names\n",
    "             example_data_1D['y'], \n",
    "             yerr=example_data_1D['error'],#yerr denotes an error in the y-direction for plotting\n",
    "             fmt='.') #fmt is \"format\", saying that I want data marked by \"points\"\n",
    "plt.xlabel('Days since I left the honey jar out') #set the x-axis label \n",
    "plt.ylabel('Number of ants') #set the y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data were generated with a simple quadratic equation:\n",
    "#ax^2+bx+c. \n",
    "def modelA_quadratic(x, alpha, beta, gamma): \n",
    "    \"\"\"A quadratic in x (this happens to be the true model)\"\"\"\n",
    "    return(alpha*x**2 + beta*x + gamma)\n",
    "\n",
    "def modelB_cubic(x, p0, p1, p2, p3):\n",
    "    \"\"\"A third-order polynomial model.\"\"\"\n",
    "    return( p0 + p1*x + p2*x**2 + p3*x**3)\n",
    "    \n",
    "def modelC_exponential(x, floor, scale, expfactor):\n",
    "    \"\"\"\"\"\"\n",
    "    return( floor + scale * np.exp( expfactor * x) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_ln_likelihoodA_quadratic(theta, args):\n",
    "    \"\"\" This function accepts an argument \"theta\", which is \n",
    "    a list of parameter values [alpha,beta,gamma] for model A.\n",
    "    It then calculates a log-likelihood by computing the \n",
    "    chi-squared statistic (i.e., assuming gaussian uncertainties), \n",
    "    which compares the observations and errors (provided in args) \n",
    "    to the model A.\n",
    "    \"\"\"   \n",
    "    x, y, yerr = args \n",
    "    alpha, beta, gamma = theta \n",
    "    \n",
    "    model_at_observed_x = modelA_quadratic(x, alpha, beta, gamma) \n",
    "    inverse_uncertainty2 = 1./yerr**2 \n",
    "    chisquared = np.sum((y - model_at_observed_x)**2 \n",
    "                        * inverse_uncertainty2 )\n",
    "    # When all uncertainties are gaussian and uncorrelated, the \n",
    "    # natural log of the likelihood is ln(likelihood) = -0.5 * chi2 \n",
    "    # We want -ln(likelihood) so we return 0.5*chi2.\n",
    "    return (0.5 * chisquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_ln_likelihoodB_cubic(theta, args):\n",
    "    \"\"\" This function accepts an argument \"theta\", which is \n",
    "    a list of parameter values [p0,p1,p2,p3] for model B \n",
    "    (the cubic model).\n",
    "    It then calculates a log-likelihood by computing the \n",
    "    chi-squared statistic (i.e., assuming gaussian uncertainties), \n",
    "    which compares the observations and errors (provided in args) \n",
    "    to the model B.\n",
    "    \"\"\"   \n",
    "    x, y, yerr = args \n",
    "\n",
    "    # Introducing a short-hand here, to pass a list \n",
    "    # as distinct arguments to a function, pass *name_of_list\n",
    "    model_at_observed_x = modelB_cubic(x, *theta) \n",
    "    \n",
    "    # that is equivalent to doing: \n",
    "    # p0, p1, p2, p3 = theta \n",
    "    # model_at_observed_x = modelB_quartic(x, p0, p1, p2, p3)\n",
    "\n",
    "    inverse_uncertainty2 = 1./yerr**2 \n",
    "    chisquared = np.sum((y - model_at_observed_x)**2 \n",
    "                        * inverse_uncertainty2 )\n",
    "    return (0.5 * chisquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_ln_likelihoodC_exponential(theta, args):\n",
    "    \"\"\" This function accepts an argument \"theta\", which is \n",
    "    a list of parameter values [floor, scale, k] for model C \n",
    "    (the exponential model).\n",
    "    It then calculates a log-likelihood by computing the \n",
    "    chi-squared statistic (i.e., assuming gaussian uncertainties), \n",
    "    which compares the observations and errors (provided in args) \n",
    "    to the model C.\n",
    "    \"\"\"   \n",
    "    x, y, yerr = args \n",
    "    \n",
    "    model_at_observed_x = modelC_exponential(x, *theta) \n",
    "    inverse_uncertainty2 = 1./yerr**2 \n",
    "    chisquared = np.sum((y - model_at_observed_x)**2 \n",
    "                        * inverse_uncertainty2 )\n",
    "    return (0.5 * chisquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what these models look like, using some parameters that are in the right ball park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_three_models(thetaA, thetaB, thetaC):\n",
    "    \"\"\" Plot all three models over the data, using \n",
    "    three parameter sets thetaA, thetaB and thetaC\"\"\"\n",
    "    # first plot the data\n",
    "    plt.errorbar(example_data_1D['x'], example_data_1D['y'], \n",
    "             yerr=example_data_1D['error'],\n",
    "             fmt='.') \n",
    "    plt.xlabel('Days since I left the honey jar out')\n",
    "    plt.ylabel('Number of ants')\n",
    "\n",
    "    # now plot the models\n",
    "    x_for_plotting = np.arange(-0.02, 10.1, 0.1)\n",
    "    plt.plot(x_for_plotting, \n",
    "             modelA_quadratic(x_for_plotting, *thetaA),\n",
    "             color='b', label='A (quadratic)')\n",
    "         \n",
    "    plt.plot(x_for_plotting, \n",
    "             modelB_cubic(x_for_plotting, *thetaB),\n",
    "             color='g', label='B (cubic)')\n",
    "\n",
    "    plt.plot(x_for_plotting, \n",
    "             modelC_exponential(x_for_plotting, *thetaC),\n",
    "             color='m', label='C (exponential)')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_three_models([50, -5, 1], [20, 10, 0.7, 4], [-100,100,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a maximum likelihood estimation (equivalent to chi2 minimization) to find the best parameters for each model\n",
    "\n",
    "maxlike_resultA = scipy.optimize.minimize(\n",
    "    neg_ln_likelihoodA_quadratic, \n",
    "    x0=[20,-1,-1],\n",
    "    bounds=None, #[(-100,100),(-100,100),(0,100)], \n",
    "    args=[example_data_1D['x'],\n",
    "          example_data_1D['y'],\n",
    "          example_data_1D['error']])\n",
    "\n",
    "maxlike_resultB = scipy.optimize.minimize(\n",
    "    neg_ln_likelihoodB_cubic, \n",
    "    x0=[20, 10, 0.7, 0.3],\n",
    "    bounds=None,\n",
    "    args=[example_data_1D['x'],\n",
    "          example_data_1D['y'],\n",
    "          example_data_1D['error']])\n",
    "\n",
    "maxlike_resultC = scipy.optimize.minimize(\n",
    "    neg_ln_likelihoodC_exponential, \n",
    "    x0=[-100, 200, 1],\n",
    "    bounds=None, #[(-np.inf,np.inf),(-np.inf,np.inf),(-np.inf, np.inf)], \n",
    "    args=[example_data_1D['x'],\n",
    "          example_data_1D['y'],\n",
    "          example_data_1D['error']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maxlike_resultA['x'])\n",
    "print(maxlike_resultB['x'])\n",
    "print(maxlike_resultC['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_three_models(maxlike_resultA['x'], \n",
    "                  maxlike_resultB['x'], \n",
    "                  maxlike_resultC['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison :  the AIC and BIC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Akaike information criterion is defined as:\n",
    "\n",
    "### AIC = 2 k - 2 ln(L)\n",
    "\n",
    "it balances a model's ability to fit the data (measured by the maximum likelihood value L) against the number of parameters 'k' that the model requires.  A smaller value of the AIC indicates a better model (i.e., one that matches the data well, without being unnecessarily complex).\n",
    "\n",
    "The Bayesian information criterion is very similar. It replaces the 2 in the first term with ln(n), where n is the number of data points.  This puts more weight on the first term (which penalizes complexity) when the size of the sample is large.  As with the AIC, smaller is better.\n",
    "\n",
    "### BIC = k ln( n ) - 2 ln( L )\n",
    "\n",
    "These two metrics are the most commonly used, but many others exist, with subtle differences in their properties.  One should take care to apply the appropriate criteria based on the data, the models, and the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define a function that computes the AIC and BIC for each of our three models\n",
    "\n",
    "def aic(numparams, lnmaxlikelihood):\n",
    "    return (2 * numparams - 2 * lnmaxlikelihood)\n",
    "\n",
    "\n",
    "def bic(numparams, numdatapoints, lnmaxlikelihood):\n",
    "    return (numparams * np.log(numdatapoints) - 2 * lnmaxlikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute each max likelihood value and report.  \n",
    "\n",
    "# Remember that we have found the minimum\n",
    "# of the negative log likelihood for each\n",
    "# function. This is reported as the 'fun'\n",
    "# entry in our set of results from the \n",
    "# scipy.optimize.minimize() function calls.\n",
    "\n",
    "# The opposite of that minimum is our maximum log likelihood.\n",
    "\n",
    "maxlikelihoodvalueA = -maxlike_resultA['fun']\n",
    "maxlikelihoodvalueB = -maxlike_resultB['fun']\n",
    "maxlikelihoodvalueC = -maxlike_resultC['fun']\n",
    "\n",
    "print(maxlikelihoodvalueA, maxlikelihoodvalueB, maxlikelihoodvalueC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the AIC and BIC for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_list = [\n",
    "    aic(3, maxlikelihoodvalueA),\n",
    "    aic(4, maxlikelihoodvalueB),\n",
    "    aic(3, maxlikelihoodvalueC)]\n",
    "\n",
    "n = len(example_data_1D)\n",
    "bic_list = [\n",
    "    bic(3, n, maxlikelihoodvalueA),\n",
    "    bic(4, n, maxlikelihoodvalueB),\n",
    "    bic(3, n, maxlikelihoodvalueC)]\n",
    "\n",
    "print(aic_list)\n",
    "print(bic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a nice pandas DataFrame table \n",
    "\n",
    "modelnames = ['A(quadratic)', 'B(cubic)', 'C(exp)']\n",
    "df = pandas.DataFrame({\n",
    "    'name':modelnames, \n",
    "    'AIC':aic_list, \n",
    "    'BIC':bic_list,\n",
    "    'DeltaAIC':np.array(aic_list)-np.min(aic_list),\n",
    "    'DeltaBIC':np.array(bic_list)-np.min(bic_list),\n",
    "})\n",
    "\n",
    "# Show the table\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreting the AIC / BIC as statistical weight\n",
    "\n",
    "wAIC = np.exp(-0.5 * df['DeltaAIC'])\n",
    "df['wgtAIC'] = wAIC / np.sum(wAIC)\n",
    "\n",
    "wBIC = np.exp(-0.5 * df['DeltaBIC'])\n",
    "df['wgtBIC'] = wBIC / np.sum(wBIC)\n",
    "\n",
    "\n",
    "# Interpreting the AIC / BIC as an odds ratio\n",
    "df['oddsAIC'] = np.max(df['wgtAIC']) / df['wgtAIC']\n",
    "df['oddsBIC'] = np.max(df['wgtBIC']) / df['wgtBIC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a pandas option so we only display two decimal places\n",
    "pandas.options.display.float_format = '{:.2f}'.format\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next topic :  using the Bayesian evidence (Bayes factors) to compare models considering the entire parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading list\n",
    "\n",
    "A good broad book on Bayesian data analysis\n",
    "* Sivia, D. and Skilling, J. \"Data Analysis: A Bayesian Tutorial\"\n",
    "https://books.google.com/books/about/Data_Analysis.html?id=lYMSDAAAQBAJ\n",
    "\n",
    "Some summary papers: \n",
    "\n",
    "* Wagenmakers and Farrell 2004\n",
    "https://link.springer.com/content/pdf/10.3758/BF03206482.pdf\n",
    "\n",
    "* Symonds and Moussalli 2010\n",
    "http://byrneslab.net/classes/biol607/readings/Symonds_and_Moussalli_2010_behav_ecol.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
